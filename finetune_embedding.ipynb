{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a8b793",
   "metadata": {
    "id": "28a8b793"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/finetuning/embeddings/finetune_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551753b7-6cd2-4f81-aec0-da119e4705ad",
   "metadata": {
    "id": "551753b7-6cd2-4f81-aec0-da119e4705ad"
   },
   "source": [
    "# Finetune Embeddings\n",
    "\n",
    "In this notebook, we show users how to finetune their own embedding models.\n",
    "\n",
    "We go through three main sections:\n",
    "1. Preparing the data (our `generate_qa_embedding_pairs` function makes this easy)\n",
    "2. Finetuning the model (using our `SentenceTransformersFinetuneEngine`)\n",
    "3. Evaluating the model on a validation knowledge corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99afd542-fc47-44ac-aed0-b3684108dba5",
   "metadata": {
    "id": "99afd542-fc47-44ac-aed0-b3684108dba5"
   },
   "source": [
    "## Generate Corpus\n",
    "\n",
    "First, we create the corpus of text chunks by leveraging LlamaIndex to load some financial PDFs, and parsing/chunking into plain text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e973679e",
   "metadata": {
    "id": "e973679e"
   },
   "outputs": [],
   "source": [
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index-embeddings-openai\n",
    "# %pip install llama-index-finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9280d438-b6bd-4ccf-a730-7c8bb3ebdbeb",
   "metadata": {
    "id": "9280d438-b6bd-4ccf-a730-7c8bb3ebdbeb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c42620",
   "metadata": {
    "id": "73c42620"
   },
   "source": [
    "Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e11b0c",
   "metadata": {
    "id": "d8e11b0c"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p 'data/10k/'\n",
    "# !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n",
    "# !wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa20e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# EY_TRAINING_DATA_DIR = './EYData/train'\n",
    "# EY_VALIDATION_DATA_DIR = './EYData/validate'\n",
    "\n",
    "# # Get a list of all files in the directory\n",
    "# train_files = os.listdir(EY_TRAINING_DATA_DIR)\n",
    "# validate_files = os.listdir(EY_VALIDATION_DATA_DIR)\n",
    "\n",
    "# # print(train_files)\n",
    "\n",
    "# # Filter files to only include PDFs\n",
    "# train_pdf_files = [f for f in train_files if f.endswith('.pdf')]\n",
    "# validate_pdf_files = [f for f in validate_files if f.endswith('.pdf')]\n",
    "\n",
    "# print(train_pdf_files)\n",
    "\n",
    "# # Split files into training and validation sets\n",
    "# TRAIN_FILES = [os.path.join(EY_TRAINING_DATA_DIR, f) for f in train_pdf_files[:len(train_pdf_files)]]\n",
    "# VAL_FILES = [os.path.join(EY_VALIDATION_DATA_DIR, f) for f in validate_pdf_files[:len(validate_pdf_files)]]\n",
    "\n",
    "# print(len(TRAIN_FILES))\n",
    "# print(len(VAL_FILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cbbe466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(TRAIN_FILES)\n",
    "# print(VAL_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e890bc-557b-4d3c-bede-3e80dfeeee18",
   "metadata": {
    "id": "c5e890bc-557b-4d3c-bede-3e80dfeeee18"
   },
   "outputs": [],
   "source": [
    "# TRAIN_FILES = [\"./data/10k/lyft_2021.pdf\"]\n",
    "# VAL_FILES = [\"./data/10k/uber_2021.pdf\"]\n",
    "\n",
    "# TRAIN_CORPUS_FPATH = \"./EYData/json/train_corpus.json\"\n",
    "# VAL_CORPUS_FPATH = \"./EYData/json/val_corpus.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da871c1-9d58-467a-92fd-06ed3d94534b",
   "metadata": {
    "id": "1da871c1-9d58-467a-92fd-06ed3d94534b"
   },
   "outputs": [],
   "source": [
    "# def load_corpus(files, verbose=False):\n",
    "#     if verbose:\n",
    "#         print(f\"Loading files {files}\")\n",
    "\n",
    "#     reader = SimpleDirectoryReader(input_files=files)\n",
    "#     docs = reader.load_data()\n",
    "#     if verbose:\n",
    "#         print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "#     parser = SentenceSplitter()\n",
    "#     nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"Parsed {len(nodes)} nodes\")\n",
    "\n",
    "#     return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f61b10de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U llama-index-readers-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53056d8b-3b4c-4364-9b07-a375aa84330b",
   "metadata": {
    "id": "53056d8b-3b4c-4364-9b07-a375aa84330b"
   },
   "source": [
    "We do a very naive train/val split by having the Lyft corpus as the train dataset, and the Uber corpus as the val dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3651c77-d085-4fbc-bb34-61f143ad6674",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "554a6636780246c8a19d1efe7a6e4786",
      "6748733283a34725ba6365f3c1fb1c1d"
     ]
    },
    "id": "d3651c77-d085-4fbc-bb34-61f143ad6674",
    "outputId": "32813680-397d-4771-ac5f-6257210e7ba2"
   },
   "outputs": [],
   "source": [
    "# train_nodes = load_corpus(TRAIN_FILES, verbose=True)\n",
    "# val_nodes = load_corpus(VAL_FILES, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "291e6680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4482c48-844b-448b-9552-3f38b455645c",
   "metadata": {
    "id": "b4482c48-844b-448b-9552-3f38b455645c"
   },
   "source": [
    "### Generate synthetic queries\n",
    "\n",
    "Now, we use an LLM (gpt-3.5-turbo) to generate questions using each text chunk in the corpus as context.\n",
    "\n",
    "Each pair of (generated question, text chunk used as context) becomes a datapoint in the finetuning dataset (either for training or evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "580334ce-ddaa-4cc0-8c3e-7294d11e4d2f",
   "metadata": {
    "id": "580334ce-ddaa-4cc0-8c3e-7294d11e4d2f"
   },
   "outputs": [],
   "source": [
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "666001e2",
   "metadata": {
    "id": "666001e2"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import getpass\n",
    "\n",
    "# if \"OPENAI_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your Open AI API key: \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a028cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "# train_dataset = generate_qa_embedding_pairs(\n",
    "#     llm=OpenAI(model=\"gpt-4o\"),\n",
    "#     nodes=train_nodes,\n",
    "#     output_path=\"train_dataset.json\",\n",
    "# )\n",
    "# val_dataset = generate_qa_embedding_pairs(\n",
    "#     llm=OpenAI(model=\"gpt-4o\"),\n",
    "#     nodes=val_nodes,\n",
    "#     output_path=\"val_dataset.json\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07e3626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.openai_like import OpenAILike\n",
    "\n",
    "# # llm = OpenAILike(model=\"neuralmagic/Llama-3.2-3B-Instruct-FP8\", api_base=\"http://localhost:8000/v1\", api_key=\"NOKEY\")\n",
    "# llm = OpenAILike(model=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\", api_base=\"http://43.230.201.125:60100/v1\", api_key=\"NOKEY\")\n",
    "# # neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\n",
    "# response = llm.complete(\"Hello World!\")\n",
    "\n",
    "# print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ef071c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = generate_qa_embedding_pairs(\n",
    "#     llm=llm,\n",
    "#     nodes=train_nodes,\n",
    "#     output_path=\"train_dataset.json\",\n",
    "# )\n",
    "# val_dataset = generate_qa_embedding_pairs(\n",
    "#     llm=llm,\n",
    "#     nodes=val_nodes,\n",
    "#     output_path=\"val_dataset.json\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "743f163c-25df-4c18-9abe-05052b034d70",
   "metadata": {
    "id": "743f163c-25df-4c18-9abe-05052b034d70"
   },
   "outputs": [],
   "source": [
    "# [Optional] Load\n",
    "train_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset.json\")\n",
    "val_dataset = EmbeddingQAFinetuneDataset.from_json(\"val_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62368cb8-a303-48b1-8429-5e3655abcc3b",
   "metadata": {
    "id": "62368cb8-a303-48b1-8429-5e3655abcc3b"
   },
   "source": [
    "## Run Embedding Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1d08066-5f00-48f1-b12a-e80bc193d4c0",
   "metadata": {
    "id": "c1d08066-5f00-48f1-b12a-e80bc193d4c0"
   },
   "outputs": [],
   "source": [
    "from llama_index.finetuning import SentenceTransformersFinetuneEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26625ab5-ddc9-4dbd-9936-39b69c6a7cdc",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6efae9c64cdc4a92a248cf1619349958",
      "82a58350abe74a59b39686504e56ddb7",
      "c938a8515da340fa8567502eb4ab1379",
      "cec7526cf6d74ab5a90b5a2adecb8dcf",
      "e8177356c92541939bdc0d7f51a88dd2",
      "1155eb2d55b446639814729da89f2a8f",
      "8db118818c7e4dacb623944d8888e0a2",
      "2c7498da6664460ab67ac5fc72fcd565",
      "dfa0168357b74c7f900e49c4cb38b4eb",
      "39c5ef016f2f4d73bd9d78e081b46f47",
      "2d40419c72754123b95d7a4f3430cac3",
      "aba92340280a4601a19f4a8707c45fba",
      "e03c93e272574b46a7bb8ca5e389b354"
     ]
    },
    "id": "26625ab5-ddc9-4dbd-9936-39b69c6a7cdc",
    "outputId": "279c80da-5765-4c0e-e88c-ff670242af79"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashutosh/miniconda3/envs/vLlamaindex/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.5.1 available.\n",
      "PyTorch version 2.5.1 available.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-m3\n",
      "Load pretrained SentenceTransformer: BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashutosh/miniconda3/envs/vLlamaindex/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "finetune_engine = SentenceTransformersFinetuneEngine(\n",
    "    train_dataset,\n",
    "    model_id=\"BAAI/bge-m3\",\n",
    "    model_output_path=\"FineTuned_Model\",\n",
    "    val_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f17cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d90fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "302f0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad99e6-dd9d-485a-86e9-1845cf51802b",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "78dab7a09dd640619d80e986baf37249",
      "b025b81ebe21403498679bf916626ff9",
      "e737aae9a5f4459c97df630e63b9c487"
     ]
    },
    "id": "28ad99e6-dd9d-485a-86e9-1845cf51802b",
    "outputId": "7a307bc7-4a0f-44df-c547-608193874cf6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1710 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a2ba2-e7e6-4025-8887-cac6e7ecb493",
   "metadata": {
    "id": "467a2ba2-e7e6-4025-8887-cac6e7ecb493"
   },
   "outputs": [],
   "source": [
    "embed_model = finetune_engine.get_finetuned_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d16ec01-c29d-4742-aa3c-5ded6ae7c5a7",
   "metadata": {
    "id": "5d16ec01-c29d-4742-aa3c-5ded6ae7c5a7",
    "outputId": "ca8a3da1-ea90-420a-f228-025762c5e994"
   },
   "outputs": [],
   "source": [
    "embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828dd6fe-9a8a-419b-8663-56d81ce73774",
   "metadata": {
    "id": "828dd6fe-9a8a-419b-8663-56d81ce73774"
   },
   "source": [
    "## Evaluate Finetuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a66b83-4cbb-4374-a632-0f1bb2b785ab",
   "metadata": {
    "id": "f4a66b83-4cbb-4374-a632-0f1bb2b785ab"
   },
   "source": [
    "In this section, we evaluate 3 different embedding models:\n",
    "1. proprietary OpenAI embedding,\n",
    "2. open source `BAAI/bge-small-en`, and\n",
    "3. our finetuned embedding model.\n",
    "\n",
    "We consider 2 evaluation approaches:\n",
    "1. a simple custom **hit rate** metric\n",
    "2. using `InformationRetrievalEvaluator` from sentence_transformers\n",
    "\n",
    "We show that finetuning on synthetic (LLM-generated) dataset significantly improve upon an opensource embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5176f-1f21-4bcb-adf5-da1c4cccb8d3",
   "metadata": {
    "id": "57d5176f-1f21-4bcb-adf5-da1c4cccb8d3"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.schema import TextNode\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda4c2b8-1ad8-420c-83d2-b88e0519895d",
   "metadata": {
    "id": "dda4c2b8-1ad8-420c-83d2-b88e0519895d"
   },
   "source": [
    "### Define eval function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c24d3-3d72-4ce8-94a4-2da9c1b2605c",
   "metadata": {
    "id": "398c24d3-3d72-4ce8-94a4-2da9c1b2605c"
   },
   "source": [
    "**Option 1**: We use a simple **hit rate** metric for evaluation:\n",
    "* for each (query, relevant_doc) pair,\n",
    "* we retrieve top-k documents with the query,  and\n",
    "* it's a **hit** if the results contain the relevant_doc.\n",
    "\n",
    "This approach is very simple and intuitive, and we can apply it to both the proprietary OpenAI embedding as well as our open source and fine-tuned embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89401d3-a157-4f96-86d4-212e631a54bc",
   "metadata": {
    "id": "b89401d3-a157-4f96-86d4-212e631a54bc"
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    dataset,\n",
    "    embed_model,\n",
    "    top_k=5,\n",
    "    verbose=False,\n",
    "):\n",
    "    corpus = dataset.corpus\n",
    "    queries = dataset.queries\n",
    "    relevant_docs = dataset.relevant_docs\n",
    "\n",
    "    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]\n",
    "    index = VectorStoreIndex(\n",
    "        nodes, embed_model=embed_model, show_progress=True\n",
    "    )\n",
    "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "\n",
    "    eval_results = []\n",
    "    for query_id, query in tqdm(queries.items()):\n",
    "        retrieved_nodes = retriever.retrieve(query)\n",
    "        retrieved_ids = [node.node.node_id for node in retrieved_nodes]\n",
    "        expected_id = relevant_docs[query_id][0]\n",
    "        is_hit = expected_id in retrieved_ids  # assume 1 relevant doc\n",
    "\n",
    "        eval_result = {\n",
    "            \"is_hit\": is_hit,\n",
    "            \"retrieved\": retrieved_ids,\n",
    "            \"expected\": expected_id,\n",
    "            \"query\": query_id,\n",
    "        }\n",
    "        eval_results.append(eval_result)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb16251-bb45-4de0-b65a-e15aa76e0f1e",
   "metadata": {
    "id": "7eb16251-bb45-4de0-b65a-e15aa76e0f1e"
   },
   "source": [
    "**Option 2**: We use the `InformationRetrievalEvaluator` from sentence_transformers.\n",
    "\n",
    "This provides a more comprehensive suite of metrics, but we can only run it against the sentencetransformers compatible models (open source and our finetuned model, *not* the OpenAI embedding model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e89702-ea35-4c22-99c7-f89a5428ef95",
   "metadata": {
    "id": "88e89702-ea35-4c22-99c7-f89a5428ef95"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def evaluate_st(\n",
    "    dataset,\n",
    "    model_id,\n",
    "    name,\n",
    "):\n",
    "    corpus = dataset.corpus\n",
    "    queries = dataset.queries\n",
    "    relevant_docs = dataset.relevant_docs\n",
    "\n",
    "    evaluator = InformationRetrievalEvaluator(\n",
    "        queries, corpus, relevant_docs, name=name\n",
    "    )\n",
    "    model = SentenceTransformer(model_id)\n",
    "    output_path = \"results/\"\n",
    "    Path(output_path).mkdir(exist_ok=True, parents=True)\n",
    "    return evaluator(model, output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d33dd-c39f-4c05-8adc-65db12163c88",
   "metadata": {
    "id": "af2d33dd-c39f-4c05-8adc-65db12163c88"
   },
   "source": [
    "### Run Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c630aa25-2395-4a8b-83cf-2885fbc862f4",
   "metadata": {
    "id": "c630aa25-2395-4a8b-83cf-2885fbc862f4"
   },
   "source": [
    "#### OpenAI\n",
    "\n",
    "Note: this might take a few minutes to run since we have to embed the corpus and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0784f-415e-4d3a-8c88-757b28b9e5df",
   "metadata": {
    "id": "61a0784f-415e-4d3a-8c88-757b28b9e5df"
   },
   "outputs": [],
   "source": [
    "ada = OpenAIEmbedding()\n",
    "ada_val_results = evaluate(val_dataset, ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc73212-fc53-48c1-b347-f5ee3a29ae82",
   "metadata": {
    "id": "ccc73212-fc53-48c1-b347-f5ee3a29ae82"
   },
   "outputs": [],
   "source": [
    "df_ada = pd.DataFrame(ada_val_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eb61bb-c287-40fe-b3c7-bbfc2d2b1b94",
   "metadata": {
    "id": "25eb61bb-c287-40fe-b3c7-bbfc2d2b1b94",
    "outputId": "dcb26fd8-0c68-4a13-cfa4-9e5dadff2846"
   },
   "outputs": [],
   "source": [
    "hit_rate_ada = df_ada[\"is_hit\"].mean()\n",
    "hit_rate_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd6c62-65a8-4f72-a67c-d0d62c92d7d1",
   "metadata": {
    "id": "a1bd6c62-65a8-4f72-a67c-d0d62c92d7d1"
   },
   "source": [
    "### BAAI/bge-small-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24454aeb-9e3e-4954-ab70-647102ed7f82",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6e9c5f0555f641caa3a5a5d11cb87583",
      "1fe9a221f8984c818727771d12dfef71",
      "619c9cae8bf24987a4d3453aa69d24b9",
      "082cfe7c9f3646948886c90f0e1f4258",
      "3ff8d7a739fc425abf24076c47c0ab29",
      "0a5344851cb14ed8a5f788cbd74a90d8",
      "eaa8bdab99244058b1df3eae12a79b20",
      "e21b1a35d6c54644be124c357852fedf",
      "927efec699ea4c929da7214eb51fc64c",
      "1c8a00d15090422181a9749e0638e883",
      "3845bc276c88482ba0e2f2fbe317dd78",
      "7ceca7b6507e42b1b3da10711b37b7ab",
      "21170e7cf0f9485a9095807a6225aa12",
      "3712232b7e064486879945c4d4ac5535",
      "ba1f47ec020447c59d008493b31e0a57"
     ]
    },
    "id": "24454aeb-9e3e-4954-ab70-647102ed7f82",
    "outputId": "18c2ad34-aaf6-4bf1-b371-034f34a03555"
   },
   "outputs": [],
   "source": [
    "bge = \"local:BAAI/bge-small-en\"\n",
    "bge_val_results = evaluate(val_dataset, bge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da27e48-1c90-4994-aac4-96b5b1638647",
   "metadata": {
    "id": "2da27e48-1c90-4994-aac4-96b5b1638647"
   },
   "outputs": [],
   "source": [
    "df_bge = pd.DataFrame(bge_val_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc4fe0-b240-4c15-9b2d-a4c79f9aaac2",
   "metadata": {
    "id": "3ddc4fe0-b240-4c15-9b2d-a4c79f9aaac2",
    "outputId": "ea66d8c4-a81d-413c-ffc5-07d42559c70a"
   },
   "outputs": [],
   "source": [
    "hit_rate_bge = df_bge[\"is_hit\"].mean()\n",
    "hit_rate_bge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c16df14-6564-41ec-8816-348675bb0fd4",
   "metadata": {
    "id": "2c16df14-6564-41ec-8816-348675bb0fd4",
    "outputId": "ba2454e9-28d4-45fc-d124-5a42ef09e9a5"
   },
   "outputs": [],
   "source": [
    "evaluate_st(val_dataset, \"BAAI/bge-small-en\", name=\"bge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd87550-f547-4b8b-b21a-f72b355e2cd7",
   "metadata": {
    "id": "1fd87550-f547-4b8b-b21a-f72b355e2cd7"
   },
   "source": [
    "### Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402dd440-1934-4778-8ff5-28e15cf1f2d3",
   "metadata": {
    "id": "402dd440-1934-4778-8ff5-28e15cf1f2d3"
   },
   "outputs": [],
   "source": [
    "finetuned = \"local:test_model\"\n",
    "val_results_finetuned = evaluate(val_dataset, finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd24643-17cb-4773-a535-77f3f8fa2d48",
   "metadata": {
    "id": "ffd24643-17cb-4773-a535-77f3f8fa2d48"
   },
   "outputs": [],
   "source": [
    "df_finetuned = pd.DataFrame(val_results_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1dccd1-bbd4-427f-a520-b1011643d83b",
   "metadata": {
    "id": "ec1dccd1-bbd4-427f-a520-b1011643d83b"
   },
   "outputs": [],
   "source": [
    "hit_rate_finetuned = df_finetuned[\"is_hit\"].mean()\n",
    "hit_rate_finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8dd38e-f13d-43e1-9802-cc94b854526b",
   "metadata": {
    "id": "9d8dd38e-f13d-43e1-9802-cc94b854526b"
   },
   "outputs": [],
   "source": [
    "evaluate_st(val_dataset, \"test_model\", name=\"finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc290bc-5cc3-4ee4-b8ab-e68371441643",
   "metadata": {
    "id": "fbc290bc-5cc3-4ee4-b8ab-e68371441643"
   },
   "source": [
    "### Summary of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f906a11-6a95-4f10-9069-140bf5a56246",
   "metadata": {
    "id": "6f906a11-6a95-4f10-9069-140bf5a56246"
   },
   "source": [
    "#### Hit rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705fbe3c-2843-4bab-bb5c-16027fc5564b",
   "metadata": {
    "id": "705fbe3c-2843-4bab-bb5c-16027fc5564b"
   },
   "outputs": [],
   "source": [
    "df_ada[\"model\"] = \"ada\"\n",
    "df_bge[\"model\"] = \"bge\"\n",
    "df_finetuned[\"model\"] = \"fine_tuned\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebc363c-cd07-4dab-916e-1618d16d1254",
   "metadata": {
    "id": "bebc363c-cd07-4dab-916e-1618d16d1254"
   },
   "source": [
    "We can see that fine-tuning our small open-source embedding model drastically improve its retrieval quality (even approaching the quality of the proprietary OpenAI embedding)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f38b4b-1b40-42da-a054-ea9593d3e602",
   "metadata": {
    "id": "57f38b4b-1b40-42da-a054-ea9593d3e602"
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_ada, df_bge, df_finetuned])\n",
    "df_all.groupby(\"model\").mean(\"is_hit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08094d07-2c0a-44ca-ad2f-8d8bf1387ed9",
   "metadata": {
    "id": "08094d07-2c0a-44ca-ad2f-8d8bf1387ed9"
   },
   "source": [
    "#### InformationRetrievalEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0444e-a824-42d6-9ddb-4da7179902bc",
   "metadata": {
    "id": "27d0444e-a824-42d6-9ddb-4da7179902bc"
   },
   "outputs": [],
   "source": [
    "df_st_bge = pd.read_csv(\n",
    "    \"results/Information-Retrieval_evaluation_bge_results.csv\"\n",
    ")\n",
    "df_st_finetuned = pd.read_csv(\n",
    "    \"results/Information-Retrieval_evaluation_finetuned_results.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0903ed3-df05-4d98-8b0a-6f352c681735",
   "metadata": {
    "id": "c0903ed3-df05-4d98-8b0a-6f352c681735"
   },
   "source": [
    "We can see that embedding finetuning improves metrics consistently across the suite of eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec1c46-5aa0-4f8a-a0c5-2553e08cceb1",
   "metadata": {
    "id": "81ec1c46-5aa0-4f8a-a0c5-2553e08cceb1"
   },
   "outputs": [],
   "source": [
    "df_st_bge[\"model\"] = \"bge\"\n",
    "df_st_finetuned[\"model\"] = \"fine_tuned\"\n",
    "df_st_all = pd.concat([df_st_bge, df_st_finetuned])\n",
    "df_st_all = df_st_all.set_index(\"model\")\n",
    "df_st_all"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "vLlamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
